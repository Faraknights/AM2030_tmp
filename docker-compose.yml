services:
  emollama-app:
    build:
      context: .
    ports:
      - "5000:5000"
    container_name: emollama-app
    depends_on:
      - ollama
      - react-ui
    environment:
      - OLLAMA_HOST=http://ollama:11434  # Used by your FastAPI app to call Ollama

  ollama:
    image: ollama/ollama
    container_name: ollama
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia
    environment:
      - OLLAMA_MODELS=llama3:8b
    command: >
      sh -c "ollama pull llama3:8b && ollama serve"

  react-ui:
    build:
      context: .
      dockerfile: ./react-ui/Dockerfile
    ports:
      - "3000:3000"
    volumes:
      - ./react-ui:/app
      - /app/node_modules
    environment:
      - NODE_ENV=development
    stdin_open: true
    tty: true

volumes:
  ollama-data:
